# Todo

## Week 1: Getting Started
### 2020-01-07
- [x] Write initial todo.

### 2020-01-10
- [ ] Send information to Nicolaas Heyning.
    - [ ] Ellis
    - [x] Stefan
    - [ ] Thomas
- [ ] Read about neural networks: https://www.3blue1brown.com/neural-networks
    - [X] Ellis
    - [x] Stefan
    - [ ] Thomas
- [ ] Understand Pytorch example: https://colab.research.google.com/drive/1arq7ZpWoO4Xw1od_RbMTHCl5IwIoYXOZ
    - [X] Ellis
    - [X] Stefan
    - [ ] Thomas
- [ ] Papers en blogs over pruning lezen.
    - [X] Ellis
    - [X] Stefan
    - [ ] Thomas

## Week 2: Getting Pytorch Down
### 2020-01-14
- [ ] Step 1 & 2.
    - [ ] 1: Implement a feedforward neural network and train it on the MNIST dataset.
    - [ ] 2: Implement a convolutional neural network and train it on the MNIST dataset.
    - [ ] When done, send Andrei mail for pruning code.
    - [ ] Optional: do the same for the CIFAR10 dataset.

### 2020-01-16
- [ ] Step 3.
    - [ ] Incorporate pruning mechanism.
    - [ ] Implement resetting function.
    
## Week 3: Perform Experiments
### 2020-01-21
- [ ] Step 4.
    - [ ] Experiment with different initialization schemas.
          - [ ] Pick some different schema's to try, figure out what they are
                and decide who does which one.
          - [ ] Answer the question: does the initialization schema of a
            network affect its robustness to pruning?

### 2020-01-23
- [ ] Step 5.
    - [ ] Determine for both of the above networks, and at varying degrees of
          sparsity levels, which schema works best and why.

## Week 4: Do Optional Things and Write Report
### 2020-01-28
- [ ] Step 6 & 7.
    - [ ] Based on the above results, is it possible, then, to construct a
          custom initialization schema that improves the robustness further?
    - [ ] Check whether the schema built on (6) generalizes to other pruning methods.

### 2020-01-28
- [ ] Presentations
    - [ ] Powerpoint (?)
    - [ ] Text

### 2020-01-31
- [ ] Final Report
